# PropMatch Backend

A robust property data scraping and management system for South African real estate, specifically focused on Cape Town properties from Property24.

## ğŸ“Š Project Overview

- **Total Properties Scraped**: 1,657 Cape Town properties
- **Success Rate**: 80.2%
- **Data Source**: Property24.com
- **Database**: Supabase PostgreSQL
- **Scraping Engine**: FireCrawl API with intelligent resume capabilities

## ğŸ—‚ï¸ Project Structure

```
Backend/
â”œâ”€â”€ scrapers/                    # Web scraping modules
â”‚   â”œâ”€â”€ smart_resume_scraper.py  # Intelligent auto-resume scraper
â”‚   â”œâ”€â”€ enhanced_property24_scraper.py  # Core Property24 scraping logic
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ database/                    # Database operations
â”‚   â”œâ”€â”€ database.py             # Supabase PostgreSQL integration
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                      # Data models
â”‚   â”œâ”€â”€ models.py               # PropertyData and POI models
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ .env                    # Environment variables (gitignored)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                        # Scraped data
â”‚   â””â”€â”€ property24_production_20250605_165810.json  # Main dataset (1,657 properties)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Environment Setup
Create `config/.env` file with your API keys:
```env
# FireCrawl API Keys (for scraping)
FIRECRAWL_API_KEY_1=fc-your-key-1
FIRECRAWL_API_KEY_2=fc-your-key-2
# ... up to 6 keys

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_DB_PASSWORD=your-database-password
SUPABASE_ANON_KEY=your-anon-key

# Scraping Configuration
MAX_CREDITS_PER_KEY=490
TOTAL_AVAILABLE_CREDITS=2940
MAX_SCRAPES_PER_MINUTE=10
```

### 3. Database Setup (Supabase)
```bash
# Setup database tables and migrate data
python -m database.database
```

### 4. Run Scraper (if needed)
```bash
# Smart resume scraper (automatically detects where to continue)
python -m scrapers.smart_resume_scraper
```

## ğŸ“š Key Components

### Scrapers
- **SmartResumeScraper**: Intelligent scraper that can resume from any interruption point
- **EnhancedProperty24Scraper**: Core scraping engine with dynamic feature extraction

### Database
- **PostgreSQL Schema**: Optimized for property data with JSON fields for complex data
- **Migration Tools**: Automatic JSON to PostgreSQL data migration
- **Connection Management**: Robust Supabase integration

### Models
- **PropertyData**: Comprehensive property data structure
- **POI**: Points of Interest around properties

## ğŸ”§ Features

### Scraping Features
- âœ… **Smart Resume**: Continue from any interruption point
- âœ… **API Key Rotation**: Automatic switching between 6 API keys
- âœ… **Rate Limiting**: Respectful scraping with delays
- âœ… **Error Handling**: Comprehensive retry logic
- âœ… **Real-time Progress**: Live tracking and logging
- âœ… **Incremental Saving**: No data loss on crashes

### Data Features
- âœ… **Rich Property Data**: 25+ fields per property
- âœ… **Points of Interest**: Nearby amenities and facilities
- âœ… **Dynamic Room Detection**: Flexible room type extraction
- âœ… **External Features**: Parking, pools, gardens, etc.
- âœ… **Financial Data**: Prices, levies, rates & taxes
- âœ… **Location Data**: Suburb, city, province mapping

### Database Features
- âœ… **PostgreSQL Integration**: Scalable relational database
- âœ… **JSON Field Support**: Complex data structures
- âœ… **Auto Migration**: JSON to database conversion
- âœ… **Data Validation**: Type checking and constraints
- âœ… **Performance Optimized**: Indexed searches

## ğŸ“ˆ Data Statistics

- **Total Properties**: 1,657
- **Property Types**: Houses, Apartments, Townhouses
- **Coverage Area**: Cape Town, Western Cape
- **Average Price**: R 4.2M
- **Date Range**: Current listings (as of June 2025)

## ğŸ” Security

- Environment variables for sensitive data
- Database credentials encrypted
- API keys rotated automatically
- No hardcoded secrets in code

## ğŸ› ï¸ Development

### Adding New Scrapers
1. Create new scraper in `scrapers/` directory
2. Inherit from `EnhancedProperty24Scraper` 
3. Add to `scrapers/__init__.py`

### Database Schema Changes
1. Update `database/database.py` models
2. Run migration script
3. Test with sample data

## ğŸ“„ License

This project is for educational and research purposes.

## ğŸ¤ Contributing

When adding new features:
1. Follow the existing folder structure
2. Add comprehensive logging
3. Include error handling
4. Update documentation
5. Test thoroughly

---

**Next Steps**: Set up Supabase integration and load data into PostgreSQL database. 